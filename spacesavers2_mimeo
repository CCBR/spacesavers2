#!/usr/bin/env python3
import sys
import os
import gzip
import textwrap
import time
import shutil
import subprocess

MINDEPTH = 3
QUOTA_TB = 20

from src.VersionCheck import version_check
from src.VersionCheck import __version__

version_check()

from src.FileDetails import FileDetails
from src.dfUnit import dfUnit
from src.Summary import Summary
from src.Summary import pathlen
from src.utils import *
from datetime import date

import argparse

def check_terminal_list(p,tlist):
    outcome = -1                    # append path to tlist
    for i,p2 in enumerate(tlist):
        if p.len < p2.len:
            if p.path in p2.path:
                outcome = -2         # path already in tlist
                return outcome
        else:
            if p2.path in p.path:
                outcome = i
                return outcome
    return outcome

def process_hh(
    uid,
    hashhash,
    hashhashsplits,
    mindepth,
    maxdepth,
    uid2uname,
    gid2gname,
    peruser_perfolder_summaries,
    user_output,
):
    for h in hashhash.keys():
        # if files have the same forward and reverse hashes but different sizes then 
        # hashes are split into multiple hashes with <underscore><splitnumber> suffix
        # being added to the bottom hash for each size
        split_required = hashhash[h].compute(
            hashhashsplits
        )  # compute if split is needed or if we have duplicates
        if split_required:
            continue  # split is required so move on to the next hash as new hashes with <underscore><splitnumber> have been created by compute and added to hashhashsplits ... deal with them there!
        # get indexes to files in the flist that belong to user with uid
        # if uid is zero, then get all file indexes
        uid_file_index = hashhash[h].get_user_file_index(uid)
        if len(uid_file_index) == 0: # user with uid has no files in this set
            continue
        oldest_index = hashhash[h].oldest_index
        foldest = hashhash[h].flist[oldest_index]
        user_owns_original = False
        if foldest.uid == uid or 0 == uid : user_owns_original = True
        uid_dup_file_index = []
        if hashhash[h].ndup_inode > 1: # there are duplicate inodes and hence there are duplicate files
            inodes_already_summerized = list()
            for i in uid_file_index:
                f = hashhash[h].flist[i]
                fpaths = f.get_paths(mindepth, maxdepth)
                if (
                    i == oldest_index
                ):  # its the original file ... not a duplicate
                    for p in fpaths:
                        peruser_perfolder_summaries[p].nnondup_files += 1
                        peruser_perfolder_summaries[p].non_dup_Bytes.append(f.size)
                        peruser_perfolder_summaries[p].non_dup_ages.append(f.mtime)
                    inodes_already_summerized.append(f.inode)  # scenario where original already has a hard-link
                else:
                    uid_dup_file_index.append(i)
                    # has the inode already been summarized
                    if f.inode in inodes_already_summerized:
                        for p in fpaths:
                            peruser_perfolder_summaries[p].ndup_files += 1 
                    else:
                        inodes_already_summerized.append(f.inode)
                        for p in fpaths:
                            peruser_perfolder_summaries[p].ndup_files+=1
                            peruser_perfolder_summaries[p].dup_Bytes.append(f.size)
                            peruser_perfolder_summaries[p].dup_ages.append(f.mtime)
        else:
            # ndup_inode == 1 .. meaning there are no duplicate inodes .. can still have multiple hard linked files
            # only count 1 file/hardlink for summary
            i = uid_file_index[0]
            f = hashhash[h].flist[i]
            fpaths = f.get_paths(mindepth, maxdepth)
            for p in fpaths:
                peruser_perfolder_summaries[p].nnondup_files += 1
                peruser_perfolder_summaries[p].non_dup_Bytes.append(f.size)
                peruser_perfolder_summaries[p].non_dup_ages.append(f.mtime)
        if args.duplicatesonly:
            if len(uid_dup_file_index) > 0: # this user has some duplicates
                out_index = [oldest_index]
                out_index.extend(uid_dup_file_index)
                user_output.write(
                    "{}\n".format(
                        hashhash[h].str_with_name(
                            uid2uname, gid2gname, out_index
                        )
                    )
                )
        else:
            out_index = []
            if user_owns_original == False:
                out_index.append(oldest_index)
            out_index.extend(uid_file_index)
            user_output.write(
                "{}\n".format(
                    hashhash[h].str_with_name(uid2uname, gid2gname, out_index)
                )
            )


def main():
    start = time.time()
    scriptname = os.path.basename(__file__)
    elog = textwrap.dedent(
        """\
    Version:
        {}
    Example:
        > spacesavers2_mimeo -f /output/from/spacesavers2_catalog -o /path/to/output/folder -d 7 -q 10
        """.format(
            __version__
        )
    )
    parser = argparse.ArgumentParser(
        description="spacesavers2_mimeo: find duplicates",
        epilog=elog,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )

    parser.add_argument(
        "-f",
        "--catalog",
        dest="catalog",
        required=True,
        type=str,
        default=sys.stdin,
        help="spacesavers2_catalog output from STDIN or from catalog file",
    )

    parser.add_argument(
        "-d",
        "--maxdepth",
        dest="maxdepth",
        required=False,
        type=int,
        default=10,
        help="folder max. depth upto which reports are aggregated ... absolute path is used to calculate depth (Default: 10)",
    )

    parser.add_argument(
        "-o",
        "--outdir",
        dest="outdir",
        required=False,
        type=str,
        default=os.getcwd(),
        help="output folder",
    )

    parser.add_argument(
        "-p",
        "--prefix",
        dest="prefix",
        required=False,
        type=str,
        default="",
        help="prefix for all output files",
    )

    parser.add_argument(
        "-q",
        "--quota",
        dest="quota",
        required=False,
        type=float,
        default=200.0,
        help="total quota of the mount eg. 200 TB for /data/CCBR",
    )

    parser.add_argument(
        "-z",
        "--duplicatesonly",
        dest="duplicatesonly",
        required=False,
        action=argparse.BooleanOptionalAction,
        help="Print only duplicates to per user output file.",
    )
    
    parser.add_argument(
        "-k",
        "--kronaplot",
        dest="kronaplot",
        required=False,
        action=argparse.BooleanOptionalAction,
        help="Make kronaplots for duplicates.(ktImportText must be in PATH!)",
    )

    parser.add_argument("-v", "--version", action="version", version=__version__)

    print_with_timestamp(
        start=start, scriptname=scriptname, string="version: {}".format(__version__)
    )

    global args
    args = parser.parse_args()
    quota = args.quota * 1024 * 1024 * 1024 * 1024

    if args.kronaplot:
        ktImportText_in_path = False
        if shutil.which("ktImportText") == None:
            sys.stderr.write("ktImportText(from kronaTools) not found in PATH. kronaplots will not be generated.\n")
        else:
            ktImportText_in_path = True

    uid2uname = dict()
    gid2gname = dict()
    hashhash = dict()
    users = set()  # list of all uids found
    users.add(0)  # 0 == all users
    groups = set()  # list of gids
    paths = set() # set of all paths possible 
    path_lens = set() # set of all path depths
    print_with_timestamp(
        start=start, scriptname=scriptname, string="Reading in catalog file..."
    )
    set_complete = True
    with open(args.catalog) as catalog:
        for l in catalog:
            fd = FileDetails()
            set_complete = fd.set(l)
            if not set_complete:
                continue
            if fd.issyml:
                continue  # ignore all symlinks
            users.add(fd.uid)
            groups.add(fd.gid)
            path_lens.add(get_file_depth(fd.apath))
            for p in fd.get_paths_at_all_depths():
                paths.add(p)
            hash = fd.xhash_top + "#" + fd.xhash_bottom
            if hash == "#":  # happens when file cannot be read
                sys.stderr.write(
                    "Cannot read file listed in catalog and will be excluded from mimeo: {} \n".format(
                        fd.apath
                    )
                )
                continue
            if not hash in hashhash:
                hashhash[hash] = dfUnit(hash)
            hashhash[hash].add_fd(fd)
    min_path_len = min(path_lens)
    max_path_len = max(path_lens)
    if args.maxdepth > max_path_len:
        mindepth = min_path_len
        maxdepth = max_path_len
    elif args.maxdepth > min_path_len:
        mindepth = min_path_len
        maxdepth = args.maxdepth
    else:
        mindepth = args.maxdepth
        maxdepth = args.maxdepth

    # filter paths by maxdepth
    paths = list(filter(lambda x: get_folder_depth(x) <= maxdepth, paths))
    paths = list(filter(lambda x: get_folder_depth(x) >= mindepth, paths))
    # convert PosixPaths list to list of strings
    paths = list(map(lambda x: str(x), paths))
    # sort paths for outfile aesthetics
    paths.sort()
    # reconvert to paths
    paths = list(map(lambda x: Path(x), paths))
    users = list(users)
    for uid in users:
        uid2uname[uid] = get_username_groupname(uid)
    for gid in groups:
        gid2gname[gid] = get_username_groupname(gid)
    print_with_timestamp(
        start=start, scriptname=scriptname, string="Done reading in catalog file!"
    )
    print_with_timestamp(
        start=start,
        scriptname=scriptname,
        string="Total Number of paths: %d" % len(paths),
    )
    print_with_timestamp(
        start=start,
        scriptname=scriptname,
        string="Total Number of users: %d" % len(users),
    )

    for uid in users:
        print_with_timestamp(
            start=start,
            scriptname=scriptname,
            string="Gathering stats for user: %s" % (uid2uname[uid]),
        )
        if args.prefix != "":
            outfilenameprefix = args.prefix + "." + get_username_groupname(uid)
        else:
            outfilenameprefix = get_username_groupname(uid)

        summaryfilepath = os.path.join(
            os.path.abspath(args.outdir), outfilenameprefix + ".mimeo.summary.txt"
        )
        useroutputpath = os.path.join(
            os.path.abspath(args.outdir), outfilenameprefix + ".mimeo.files.gz"
        )
        if args.kronaplot:
            kronatsv = os.path.join(
                os.path.abspath(args.outdir), outfilenameprefix + ".mimeo.krona.tsv"
            )
            if ktImportText_in_path:
                kronahtml = os.path.join(
                    os.path.abspath(args.outdir), outfilenameprefix + ".mimeo.krona.html"
                )

        with open(summaryfilepath, "w") as user_summary:
            user_summary.write("%s\n" % (Summary.HEADER))

        with gzip.open(useroutputpath, "wt") as user_output, open(
            summaryfilepath, "a"
        ) as user_summary:
            peruser_perfolder_summaries = dict()
            for p in paths:
                peruser_perfolder_summaries[p] = Summary(p)
            hashhashsplits = dict()  # dict to collect instances where the files are NOT duplicates has same hashes but different sizes (and different inodes) ... new suffix is added to bottomhash .."_iterator"
            process_hh(
                uid,
                hashhash,
                hashhashsplits,
                mindepth,
                maxdepth,
                uid2uname,
                gid2gname,
                peruser_perfolder_summaries,
                user_output,
            )
            if len(hashhashsplits) != 0: 
                hashhashsplitsdummy = dict()
                process_hh(
                    uid,
                    hashhashsplits,
                    hashhashsplitsdummy,
                    mindepth,
                    maxdepth,
                    uid2uname,
                    gid2gname,
                    peruser_perfolder_summaries,
                    user_output,
                )
                del hashhashsplitsdummy
            del hashhashsplits
            for p in paths:
                peruser_perfolder_summaries[p].update_scores(quota)
                user_summary.write(f"{peruser_perfolder_summaries[p]}\n")
        
        if args.kronaplot:
            terminal_paths = []
            with open(summaryfilepath,'r') as infile:
                count = 0
                for l in infile:
                    l = l.strip().split("\t")
                    count += 1
                    if count==1:
                        continue        #header
                    if count==2:
                        terminal_paths.append(pathlen(l[0],int(l[2])))
                        continue
                    p = pathlen(l[0],int(l[2]))
                    outcome = check_terminal_list(p,terminal_paths)
                    if outcome == -1: # new ... append
                        terminal_paths.append(p)
                    elif outcome == -2: # already in list .. move on
                        continue 
                    elif outcome > -1: # better than current one in list ... swap
                        terminal_paths[outcome] = p
            with open(kronatsv,'w') as ktsv:
                for p in terminal_paths:
                    path = p.path
                    path = path.replace('/','\t')
                    path = path.replace('\t\t','\t')
                    if p.dupbytes != 0 :
                        ktsv.write("%d\t%s\n"%(p.dupbytes,path))
            if ktImportText_in_path:
                cmd = "ktImportText %s -o %s"%(kronatsv,kronahtml)
                srun = subprocess.run(cmd,shell=True, capture_output=True, text=True)
                if srun.returncode !=0:
                    sys.stderr.write("%s\n"%(srun.stderr))    

    del hashhash
    print_with_timestamp(start=start, scriptname=scriptname, string="Finished!")


if __name__ == "__main__":
    main()
