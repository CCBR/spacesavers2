#!/usr/bin/env python3
import sys
import os
import copy
import gzip
import textwrap

MINDEPTH = 3
QUOTA_TB = 20

from src.VersionCheck import version_check
from src.VersionCheck import __version__
version_check()

from src.FileDetails import FileDetails
from src.dfUnit import dfUnit
from src.Summary import Summary

import argparse

def get_path_at_depth(path,depth):
    abspath = path.strip("/").split("/")
    maxd = len(abspath)
    if depth > maxd:
        depth = maxd
    return "/"+"/".join(abspath[:depth])

def get_paths_at_all_depths(path,maxdepth):
    paths = set()
    for i in range(MINDEPTH,maxdepth+1): 
        paths.add(get_path_at_depth(path,i))
    return paths

  



# def _append_aflist(aflist,flist):
#     for f in flist:
#         fp = f.get_path_at_depth


def main():
    elog=textwrap.dedent("""\
    Version: 
        {}
    Example: 
        % spacesavers2_finddup -f /output/from/spacesavers2_ls -o /path/to/output/folder -d 7 -q 10
        """.format(__version__)) 
    parser = argparse.ArgumentParser(description="spacesavers2_finddup: find duplicates",
                                     epilog=elog)
    parser.add_argument("-f","--lsout",dest="lsout",required=True,type=str,default=sys.stdin,
        help="spacesavers2_ls output from STDIN or from file")
    parser.add_argument("-u","--uid",dest="uid",required=False,type=int,default=0,
        help="user id or 0 for all users ... if 0 is provided the {sys.argv[1]} is run for all users")
    parser.add_argument("-d","--depth",dest="depth",required=False,type=int,default=3,
        help="folder max. depth upto which reports are aggregated")
    parser.add_argument("-o","--outdir",dest="outdir",required=False,type=str,default=os.getcwd(),
        help="output folder")
    parser.add_argument("-q","--quota",dest="quota",required=False,type=float,default=200.0,
        help="total quota of the mount eg. 200 TB for /data/CCBR")

    global args
    args = parser.parse_args()
    quota = args.quota * 1024 * 1024 * 1024 * 1024
    
    if args.depth < 3: args.depth = 3
    
    hh = dict()
    users = set()       # list of all users found
    users.add(0)        # 0 == all users
    paths = set()       # aggregate folder list .. say at depth 3 .. includes depth 2 and 1
    with open(args.lsout) as lsout:
        for l in lsout:
            fd = FileDetails()
            fd.set(l)
            if fd.issyml: continue
            users.add(fd.uid)
            paths.add(fd.get_path_at_depth(args.depth))
            hash = fd.xhash_top + "#" + fd.xhash_bottom
            if not hash in hh: hh[hash] = dfUnit(hash)
            hh[hash].flist.append(fd)
    
    allpaths = set()
    if args.depth == 3: 
        allpaths = copy.deepcopy(paths)
    else:
        for p in paths:
            allpaths = allpaths.union(get_paths_at_all_depths(p,args.depth))
    extrapaths = list(allpaths - paths)
    extrapaths.sort()
    paths = list(paths)
    paths.sort()
    
    peruser_perfolder_summaries = dict()
    for u in users:
        peruser_perfolder_summaries[u] = dict()
        for p in allpaths:
            peruser_perfolder_summaries[u][p] = Summary(p)

    for u in users:
        hashhash = copy.deepcopy(hh)
        with gzip.open(os.path.join(os.path.abspath(args.outdir),str(u)+".files.gz"),'wt',compresslevel=9) as user_output:

            hashhashsplits = dict()
            for h in hashhash.keys():
                hashhash[h].compute(args.uid,hashhashsplits)
                if hashhash[h].ndup != 0:
                    for p in paths:
                        for f in hashhash[h].flist:
                            if f.apath.startswith(p):
                                if hashhash[h].oinode != -1 and hashhash[h].oinode != f.inode: #its a duplicate
                                    peruser_perfolder_summaries[u][p].dup_Bytes.append(f.size)
                                    peruser_perfolder_summaries[u][p].dup_ages.append(f.mtime)
                                else:
                                    peruser_perfolder_summaries[u][p].non_dup_Bytes.append(f.size)
                                    peruser_perfolder_summaries[u][p].non_dup_ages.append(f.mtime)
                    user_output.write(f"{hashhash[h]}\n")

            hashhashsplitsdummy = dict()
            for h in hashhashsplits.keys():
                hashhashsplits[h].compute(args.uid,hashhashsplitsdummy)
                if hashhashsplits[h].ndup != 0:
                    for p in paths:
                        for f in hashhashsplits[h].flist:
                            if f.apath.startswith(p):
                                if hashhashsplits[h].oinode != -1 and hashhashsplits[h].oinode != f.inode: #its a duplicate
                                    peruser_perfolder_summaries[u][p].dup_Bytes.append(f.size)
                                    peruser_perfolder_summaries[u][p].dup_ages.append(f.mtime)
                                else:
                                    peruser_perfolder_summaries[u][p].non_dup_Bytes.append(f.size)
                                    peruser_perfolder_summaries[u][p].non_dup_ages.append(f.mtime)
                    user_output.write(f"{hashhashsplits[h]}\n")
        del hashhash
        del hashhashsplits
        del hashhashsplitsdummy
        user_output.close()
        with open(os.path.join(os.path.abspath(args.outdir),str(u)+".summary.txt"),'w') as user_summary:
            for p in paths:
                peruser_perfolder_summaries[u][p].update_scores(quota)
                user_summary.write(f"{peruser_perfolder_summaries[u][p]}\n")
        user_summary.close()


if __name__ == '__main__': main()
