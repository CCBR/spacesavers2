#!/usr/bin/env python3
import sys
import os
import gzip
import textwrap
import pickle
import subprocess
import shlex

MINDEPTH = 3
QUOTA_TB = 20

from src.VersionCheck import version_check
from src.VersionCheck import __version__
version_check()

from src.FileDetails import FileDetails
from src.dfUnit import dfUnit
from src.Summary import Summary
from src.utils import *

import argparse

def main():
    elog=textwrap.dedent("""\
    Version: 
        {}
    Example: 
        % spacesavers2_finddup -f /output/from/spacesavers2_ls -o /path/to/output/folder -d 7 -q 10
        """.format(__version__)) 
    parser = argparse.ArgumentParser(description="spacesavers2_finddup: find duplicates",
                                     epilog=elog)
    parser.add_argument("-f","--lsout",dest="lsout",required=True,type=str,default=sys.stdin,
        help="spacesavers2_ls output from STDIN or from file")
    parser.add_argument("-u","--uid",dest="uid",required=False,type=int,default=0,
        help="user id or 0 for all users ... if 0 is provided the {sys.argv[1]} is run for all users")
    parser.add_argument("-d","--maxdepth",dest="maxdepth",required=False,type=int,default=10,
        help="folder max. depth upto which reports are aggregated")
    parser.add_argument("-o","--outdir",dest="outdir",required=False,type=str,default=os.getcwd(),
        help="output folder")
    parser.add_argument("-q","--quota",dest="quota",required=False,type=float,default=200.0,
        help="total quota of the mount eg. 200 TB for /data/CCBR")
    parser.add_argument("-n","--parallelj",dest="parallelj",required=False,type=int,default=4,
        help="Number of users to run in parallel")
    parser.add_argument('-z',"--duplicatesonly",dest="duplicatesonly",required=False,action=argparse.BooleanOptionalAction,
        help="Print only duplicates to per user output file.")
    parser.add_argument('-p',"--peruser",dest="peruser",required=False,action=argparse.BooleanOptionalAction,
        help="By default it ignores userid and runs all at once. In addition, this option will run on per-user basis using gnuparallel")
    parser.add_argument("-t","--tmpdir",dest="tmpdir",required=False,type=str,default=os.getcwd(),
        help="temp folder to save intermediate files like the pickle")
    parser.add_argument("-c","--createpickle",dest="createpickle",required=False,type=str,
        help="create a pickle file for future use")
    parser.add_argument("-k","--readpickle",dest="readpickle",required=False,type=str,
        help="use this pickle file instead of re-reading file at -f")
        
    global args
    args = parser.parse_args()
    quota = args.quota * 1024 * 1024 * 1024 * 1024

    check_writeable_folder(args.outdir)
    # check_writeable_folder(args.tmpdir)

    if args.peruser: # check if parallel is in path
        path_to_parallel = which("parallel")
        if path_to_parallel == None:
            exit("parallel is NOT in path!")
        else:
            print(f"Using parallel from here:{path_to_parallel}")
        if args.uid != 0:
            print("ERROR: -p option can only be used with -u = 0!")
        if not args.createpickle: 
            pkl = os.path.join(os.path.abspath(args.tmpdir),"pickle.pkl")
    
    if args.createpickle and args.readpickle:
        exit("ERROR: pickle can either be created or read ... not both at the same time!")
    
    if args.createpickle:
        if os.path.exists(args.createpickle): os.remove(args.createpickle)
        pkl = args.createpickle

    if args.readpickle:
        if not os.path.exists(args.readpickle):
            exit("ERROR: {} pickle does not exist!".format(args.readpickle))
        elif not check_readable_file(args.readpickle):
            exit("ERROR: {} pickle exists but cannot be read!".format(args.readpickle))
        with open(args.readpickle, 'rb') as inpkl:
            hashhash, users, paths, uid2uname, gid2gname = pickle.load(inpkl)
    else:
        uid2uname = dict()
        gid2gname = dict()
        hashhash = dict()
        users = set()       # list of all users found
        users.add(0)        # 0 == all users
        groups = set()      # list of groups
        paths = set()       
        with open(args.lsout) as lsout:
            for l in lsout:
                fd = FileDetails()
                fd.set(l)
                if fd.issyml: continue
                users.add(fd.uid)
                groups.add(fd.gid)
                paths.add(fd.get_path())
                hash = fd.xhash_top + "#" + fd.xhash_bottom
                if not hash in hashhash: hashhash[hash] = dfUnit(hash)
                hashhash[hash].flist.append(fd)
        paths = list(map(lambda x:str(x),paths))
        paths.sort()
        paths = list(filter(lambda x:get_depth(x) <= args.maxdepth,paths))
        users = list(users)
        for uid in users:
            uid2uname[uid] = get_username_groupname(uid)
        for gid in groups:
            gid2gname[gid] = get_username_groupname(gid)

        if args.createpickle or args.peruser:
            with open(pkl,'wb') as outpkl:
                pickle.dump([hashhash,users,paths,uid2uname,gid2gname],outpkl)


    if not args.uid in users:
        exit("ERROR: {} is not a valid user!".format(str(args.uid)))

    peruser_perfolder_summaries = dict()
    for p in paths:
        peruser_perfolder_summaries[p] = Summary(p)

    with gzip.open(os.path.join(os.path.abspath(args.outdir),get_username_groupname(args.uid)+".files.gz"),'wt') as user_output:
        hashhashsplits = dict() # dict to collect instances where the files are NOT duplicates has same hashes but different sizes (and different inodes) ... new suffix is added to bottomhash .."_iterator"
        for h in hashhash.keys():
            hashhash[h].compute(args.uid,hashhashsplits)
            if hashhash[h].ndup != 0:
                for p in paths:
                    for f in hashhash[h].flist:
                        # if f.apath.startswith(p):
                        if p in str(f.apath):
                            if hashhash[h].oinode != -1 and hashhash[h].oinode != f.inode: #its a duplicate
                                peruser_perfolder_summaries[p].dup_Bytes.append(f.size)
                                peruser_perfolder_summaries[p].dup_ages.append(f.mtime)
                            else:
                                peruser_perfolder_summaries[p].non_dup_Bytes.append(f.size)
                                peruser_perfolder_summaries[p].non_dup_ages.append(f.mtime)
                if args.duplicatesonly:
                    if hashhash[h].ndup > 1:
                        user_output.write("{}\n".format(hashhash[h].str_with_name(uid2uname,gid2gname)))
                else:
                    user_output.write("{}\n".format(hashhash[h].str_with_name(uid2uname,gid2gname)))

        hashhashsplitsdummy = dict()
        for h in hashhashsplits.keys():
            hashhashsplits[h].compute(args.uid,hashhashsplitsdummy)
            if hashhashsplits[h].ndup != 0:
                for p in paths:
                    for f in hashhashsplits[h].flist:
                        # if f.apath.startswith(p):
                        if p in str(f.apath):
                            if hashhashsplits[h].oinode != -1 and hashhashsplits[h].oinode != f.inode: #its a duplicate
                                peruser_perfolder_summaries[p].dup_Bytes.append(f.size)
                                peruser_perfolder_summaries[p].dup_ages.append(f.mtime)
                            else:
                                peruser_perfolder_summaries[p].non_dup_Bytes.append(f.size)
                                peruser_perfolder_summaries[p].non_dup_ages.append(f.mtime)
                if args.duplicatesonly:
                    if hashhashsplits[h].ndup > 1:
                        user_output.write("{}\n".format(hashhashsplits[h].str_with_name(uid2uname,gid2gname)))
                else:
                    user_output.write("{}\n".format(hashhashsplits[h].str_with_name(uid2uname,gid2gname)))
    del hashhash
    del hashhashsplits
    del hashhashsplitsdummy
    # user_output.close()
    with open(os.path.join(os.path.abspath(args.outdir),get_username_groupname(args.uid)+".summary.txt"),'w') as user_summary:
        for p in paths:
            peruser_perfolder_summaries[p].update_scores(quota)
            user_summary.write(f"{peruser_perfolder_summaries[p]}\n")
    # user_summary.close()

    if args.peruser:
        parallelfilename = os.path.join(os.path.abspath(args.outdir),"do_all_users")
        with open(parallelfilename,'w') as parallelfile:
            for u in users:
                if u == 0: continue
                cmd = __file__
                cmd += " -f "+args.lsout
                cmd += " -u "+str(u)
                cmd += " -d "+str(args.maxdepth)
                cmd += " -o "+args.outdir
                cmd += " -q "+str(args.quota)
                cmd += " -k "+pkl
                if args.duplicatesonly: cmd += " -z "
                parallelfile.write("{}\n".format(cmd))
        parallelcmd = "parallel -j "+str(args.parallelj)+" -a "+parallelfilename
        # print(parallelcmd)
        subprocess.run(shlex.split(parallelcmd,posix=True),shell=False,text=True)

if __name__ == '__main__': main()
